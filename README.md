# On Optimal Transformer Depth for Low-Resource Language Translation

## Code as of April 2020

### MasakhaneMT
This work was done as a part of the [MasakhaneMT](https://www.masakhane.io/) initiative.
As such, we make heavy use of the tools and code developed by the community.
Farther more, it is probable that this work will be merged into that [code base](https://github.com/masakhane-io/masakhane) and continued from there.
Thus, be sure to check there for the most recent form of this work.

## Paper

### arXiv
The full paper is available on [arXiv](https://arxiv.org/abs/2004.04418).

### SlidesLive presentation
This work is presented as part of the [AfricaNLP ICLR 2020 workshop](https://africanlp-workshop.github.io/).
As such, it is presented online via [SlidesLive](https://slideslive.com/38926580/on-optimal-transformer-depth-for-lowresource-language-translation?ref=account-folder-46632-folders).
While there, please have a look at the [other work](https://slideslive.com/iclr-2020/workshop-africanlp-unlocking-local-languages) that was presented at the AfricaNLP workshop!

### Poster
We have also created a [Poster](https://drive.google.com/file/d/1YfZie3P9s1OWA3gkKAzS-QNzFgqKFVRg/view?usp=sharing) summary of this work.

## Dataset
The data used in this work was left exactly as processed by Jade Abbott and Laura Martinus and can be found on their [GitHub repository](https://github.com/LauraMartinus/ukuxhumana) (Code to download this can be found inside each of the notebooks in this repository).
